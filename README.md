# LLM Fine-Tuning Experiments

Some experiments of mine with large-language-model fine-tuning.  
This small project was mainly a personal playground to explore what’s possible on limited hardware.

## What’s Inside
- Tests of fine-tuning **gpt-oss-20b** quantized in **4-bit (MXFP4)** and **Phi-3-mini-4k-instruct-4bit**  
- Exploration of **GRPO** (a reinforcement-learning–based optimization technique) for lightweight model adaptation  
- Runs executed on a **Mac Pro M1 with 16 GB of unified memory**, just to see how far such hardware can be pushed

## Notes
These are just experiments—please use at your own risk.  
I built this mostly out of curiosity, and I’m sharing it online in case it’s useful or interesting to others.
